{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latural Language Processing with RNN\n",
        "By: Matthew Fernandez\n",
        "<br>01/12/2022"
      ],
      "metadata": {
        "id": "qog5ASTYPi6p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAeAdFUnWIAO",
        "outputId": "0dfa3179-98d9-4f55-c17b-c55d8f79888f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "We will look at the shakespeare dataset found in tf.keras.utils package"
      ],
      "metadata": {
        "id": "vocVrb8BXNk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file(\"shakespeare.txt\", \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\")"
      ],
      "metadata": {
        "id": "T6gi5BwcWM8r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading your own data"
      ],
      "metadata": {
        "id": "9i8SUahpXZmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "a7WOLPJOWo_u"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av-SwojdXmop",
        "outputId": "0ca5fbaa-d452-4c72-e11b-86f0218880a4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADrU2GDdYLBo",
        "outputId": "fd0b5e1f-b561-48b2-b92b-f246e16085b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "dYA2lobTYYYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i,u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "8JsiwNxiYRNW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Text: \", text[:13])\n",
        "print(\"Encoded: \",text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEAhHZEAY99c",
        "outputId": "3f25d834-8fdb-4559-cde5-232856127a4d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text:  First Citizen\n",
            "Encoded:  [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert our numeric values to text."
      ],
      "metadata": {
        "id": "zS4ifseS-CkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return \"\".join(idx2char[ints])"
      ],
      "metadata": {
        "id": "ADz7EmQzZMw2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Examples\n",
        "The training examples we will prepare will use *sequence_length* as the input and *sequence_length* sequence as the output where that sequence is shifted one to the right.<br>Example:\n",
        "- Input: Hell | output: ello"
      ],
      "metadata": {
        "id": "tAivBvD5-SxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 100\n",
        "examples_per_epoch =len(text)//(sequence_length+1)\n",
        "\n",
        "#create tr examples from dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "Bo21XXSP-BlU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next let us use the batch method to turn this stream of characters into batches of desired length."
      ],
      "metadata": {
        "id": "Z7wdMouPBSsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "weM3tjUR-j5b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to use these sequwnce of length 101 and split into input and output."
      ],
      "metadata": {
        "id": "9cCmxbT8BZu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:-1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "LKPk4i3b-2tj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in dataset.take(2):\n",
        "  print('\\n\\nExample\\nInput')\n",
        "  print(int_to_text(x))\n",
        "  print('\\nOutput')\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E6JvS89_KTM",
        "outputId": "c717e8f7-5c77-4c2e-c686-fc1d85dcbc19"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Example\n",
            "Input\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "Output\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "Example\n",
            "Input\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "Output\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, create the training batches."
      ],
      "metadata": {
        "id": "Zq5VLOqgBiAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = len(vocab) # num of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "HIy_rUFt_eld"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model\n",
        "Now we build the model. Let use use an embedding layer LSTM and one dense layer that contains a node for each unique character in our training data. The dense layer will give use a probability distribution over all nodes."
      ],
      "metadata": {
        "id": "_y4pmsQhC1Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size,None]),\n",
        "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3C7NJ9fAAkO",
        "outputId": "94d282ad-ce7d-45d4-92eb-88c9172881b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 256)          16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (128, None, 1024)         5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 65)           66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Loss Function\n",
        "We need a loss function, so we'll build our own. This is because we want our output to be (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character at each timestamp for every sequence in the batch."
      ],
      "metadata": {
        "id": "i2Su9NfkDQgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"  (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncehn7koCwXt",
        "outputId": "b1926bc2-76dc-426a-c62c-5882dc3d5159"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 100, 65)   (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhE_C9J5EXwv",
        "outputId": "dfe64b69-8a0c-4e89-e45b-b28a3f6db89a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128\n",
            "tf.Tensor(\n",
            "[[[-1.0092584e-03 -3.7219739e-03  3.6222248e-03 ...  3.1431572e-04\n",
            "    9.9778106e-04 -5.7588527e-03]\n",
            "  [-7.0723980e-03 -7.9825129e-03 -8.8412990e-04 ...  2.0120195e-03\n",
            "    3.1886706e-05 -5.4056882e-03]\n",
            "  [-7.5174035e-03 -4.9794763e-03 -8.7329478e-04 ... -1.5632802e-03\n",
            "    1.6230833e-03 -7.3378365e-03]\n",
            "  ...\n",
            "  [ 5.3603156e-03  2.5960547e-03  9.9959010e-03 ... -5.9562614e-03\n",
            "    2.6964322e-03  1.8822802e-02]\n",
            "  [ 4.8382534e-03  1.2634785e-03  1.1980202e-02 ... -4.9365051e-03\n",
            "    4.5961039e-03  1.6664194e-02]\n",
            "  [ 2.8009624e-03 -3.3238721e-03  7.5042182e-05 ... -8.6177196e-03\n",
            "    7.4084140e-03  1.5664224e-02]]\n",
            "\n",
            " [[-1.1890118e-03 -2.4077769e-04  4.4395742e-03 ... -3.9260904e-03\n",
            "   -5.2606664e-03 -5.0649201e-03]\n",
            "  [-3.7055933e-03 -6.4772303e-04  7.0612034e-04 ... -2.7532554e-03\n",
            "   -1.4399553e-03 -9.5414501e-03]\n",
            "  [-4.6190517e-03 -3.6590232e-03  4.4367774e-03 ... -9.8301144e-04\n",
            "    3.1589647e-05 -1.3350913e-02]\n",
            "  ...\n",
            "  [-7.0186877e-03 -4.5324662e-03 -8.7617716e-04 ...  2.5659630e-03\n",
            "    6.1624651e-03 -6.9738245e-03]\n",
            "  [-3.1314394e-03  3.0339574e-03  7.4583613e-03 ...  5.6051086e-03\n",
            "    5.9247166e-03 -3.1296487e-03]\n",
            "  [-8.1384368e-03 -1.6776025e-03  2.0439408e-03 ...  6.2213130e-03\n",
            "    3.8978157e-03 -3.0863085e-03]]\n",
            "\n",
            " [[-3.9515481e-03 -2.4585582e-03  3.5862208e-03 ...  3.0167417e-03\n",
            "   -2.6803960e-03 -4.5250250e-05]\n",
            "  [-4.7090282e-03 -7.5794244e-04  3.7989512e-03 ... -2.1598589e-04\n",
            "   -1.0980713e-03 -2.6031493e-03]\n",
            "  [-1.6446528e-04  5.3961696e-03  9.7908517e-03 ... -6.0106735e-03\n",
            "   -1.7708675e-03 -2.6225722e-03]\n",
            "  ...\n",
            "  [ 5.1269843e-03 -1.7254780e-03  4.2609349e-03 ... -5.3043282e-03\n",
            "    9.5572742e-03  5.2623372e-03]\n",
            "  [ 5.5481875e-03  1.5965673e-03  3.3904500e-03 ... -8.6791581e-03\n",
            "    6.0512251e-03  3.9050174e-03]\n",
            "  [ 5.8720852e-03  4.5839367e-03  2.4307976e-03 ... -1.1283697e-02\n",
            "    3.6857624e-03  2.4761001e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 2.2697619e-03 -3.0116939e-03  6.1300583e-03 ... -3.9379373e-03\n",
            "    5.2845065e-04  1.1463698e-02]\n",
            "  [-2.5715446e-03 -7.1719373e-03  6.6520530e-03 ...  6.9956196e-04\n",
            "   -1.2761843e-03  9.3274340e-03]\n",
            "  [-4.3261829e-03 -3.6215270e-03  4.4524195e-03 ... -1.7120506e-03\n",
            "    7.1797811e-04  4.7626402e-03]\n",
            "  ...\n",
            "  [-5.5213035e-03 -3.0142447e-04  4.6656462e-03 ... -9.5820928e-04\n",
            "    3.7071246e-04  6.7167031e-03]\n",
            "  [ 8.9197792e-04  5.8850870e-03  7.0501012e-03 ...  1.7846933e-03\n",
            "    3.4427298e-03  6.8284720e-03]\n",
            "  [ 2.2458324e-04 -1.3465639e-03  9.1742016e-03 ...  4.7584524e-04\n",
            "    4.4453656e-03  2.1128397e-04]]\n",
            "\n",
            " [[-2.5443167e-03  5.3259887e-04 -2.1108612e-03 ...  2.6161862e-03\n",
            "   -2.7275921e-03  1.9267426e-03]\n",
            "  [ 3.9239848e-04  7.0389551e-03  4.8708990e-03 ... -4.8840134e-03\n",
            "   -4.0141800e-03  9.0835273e-04]\n",
            "  [-8.9751766e-04  5.2137394e-03  7.6687159e-03 ... -8.1497096e-03\n",
            "   -8.4757190e-03 -5.5566700e-03]\n",
            "  ...\n",
            "  [ 3.2724084e-03 -6.1317487e-03  1.1611900e-02 ...  2.7323852e-03\n",
            "    7.3155286e-03  2.6266519e-03]\n",
            "  [-2.7904056e-03 -1.1161350e-02  4.8957500e-03 ...  4.6647852e-03\n",
            "    6.2307301e-03  1.2681407e-03]\n",
            "  [-3.6321604e-03 -8.3831223e-03  3.3360417e-03 ...  8.3630788e-04\n",
            "    7.0629581e-03 -1.9889453e-03]]\n",
            "\n",
            " [[-2.8935338e-03  2.3089142e-03 -3.4841727e-03 ...  2.9331686e-03\n",
            "    2.0722223e-03  2.8595014e-04]\n",
            "  [-6.2409826e-03 -3.5176319e-03 -1.1563090e-03 ...  5.2200090e-03\n",
            "   -8.9804467e-05  1.3538952e-03]\n",
            "  [-6.8954583e-03 -9.5750857e-04 -1.8007323e-03 ...  1.4358878e-03\n",
            "    1.6779203e-03 -9.4129326e-04]\n",
            "  ...\n",
            "  [-3.5096363e-03 -7.2903614e-03 -2.3260990e-03 ...  4.5068792e-04\n",
            "    3.1863558e-03 -1.1624816e-02]\n",
            "  [ 4.3665225e-04  7.6049665e-04  6.0138600e-03 ...  3.4254633e-03\n",
            "    3.0677405e-03 -7.0935572e-03]\n",
            "  [-4.6191309e-03 -3.6430575e-03  5.3324696e-04 ...  4.1159233e-03\n",
            "    1.2457788e-03 -6.5187868e-03]]], shape=(128, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us examine one prediction"
      ],
      "metadata": {
        "id": "W8mbQ-n1EqbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSpuwKnqEcyR",
        "outputId": "86a9eab7-2a85-494f-85bb-48015d8c5ad9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-1.0092584e-03 -3.7219739e-03  3.6222248e-03 ...  3.1431572e-04\n",
            "   9.9778106e-04 -5.7588527e-03]\n",
            " [-7.0723980e-03 -7.9825129e-03 -8.8412990e-04 ...  2.0120195e-03\n",
            "   3.1886706e-05 -5.4056882e-03]\n",
            " [-7.5174035e-03 -4.9794763e-03 -8.7329478e-04 ... -1.5632802e-03\n",
            "   1.6230833e-03 -7.3378365e-03]\n",
            " ...\n",
            " [ 5.3603156e-03  2.5960547e-03  9.9959010e-03 ... -5.9562614e-03\n",
            "   2.6964322e-03  1.8822802e-02]\n",
            " [ 4.8382534e-03  1.2634785e-03  1.1980202e-02 ... -4.9365051e-03\n",
            "   4.5961039e-03  1.6664194e-02]\n",
            " [ 2.8009624e-03 -3.3238721e-03  7.5042182e-05 ... -8.6177196e-03\n",
            "   7.4084140e-03  1.5664224e-02]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us look at prediction at first time stamp"
      ],
      "metadata": {
        "id": "XEkncZXRE8zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO9C4sFPEvB0",
        "outputId": "64c9f8b8-ebd4-4f10-da08-5da2097747c1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[-1.0092584e-03 -3.7219739e-03  3.6222248e-03 -1.6570165e-03\n",
            " -2.4570862e-03  4.7434564e-03  1.0907461e-04  1.6431680e-03\n",
            " -3.5936923e-03 -1.7743605e-03 -5.4320302e-03 -3.7677060e-03\n",
            "  6.2220235e-04 -9.3049987e-04 -8.8832271e-04  6.1103408e-03\n",
            "  3.1768254e-04 -1.6376799e-03 -1.5795175e-03 -4.7996547e-03\n",
            "  1.6170249e-03  1.2038435e-03  2.9213848e-03  1.7214839e-03\n",
            " -2.6685232e-03 -2.1070004e-03  1.3498655e-03  2.4168047e-03\n",
            "  4.1967644e-03  4.9583609e-03  7.6959790e-03 -2.6878684e-03\n",
            "  3.4799748e-03  3.5690356e-04 -1.7997827e-03  2.3036327e-03\n",
            "  5.7875604e-04  1.9788041e-03 -1.6396760e-03 -6.4158160e-04\n",
            " -5.6815532e-04 -4.7555743e-03 -2.1985534e-03  1.5205023e-03\n",
            " -3.1362816e-03 -6.1548073e-03  4.0063350e-03  6.5273414e-03\n",
            " -3.5824648e-03 -1.6087187e-03 -1.2576975e-03  2.6948140e-03\n",
            "  5.8152410e-04  6.4627267e-05 -1.1353671e-03  3.5656723e-03\n",
            " -1.8107500e-03  2.6086663e-04  1.4049106e-03  3.8930003e-03\n",
            "  4.8779380e-03  3.8774363e-03  3.1431572e-04  9.9778106e-04\n",
            " -5.7588527e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to sample the output distribution\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# we now reshape that array and convert all the ints to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "# this is what the model predicted for training sequence 1\n",
        "predicted_chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jf4i8WSaFA8i",
        "outputId": "fde7e300-235b-40aa-8b8c-5def805423f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"qJtb.C\\n:JHRhv$bnfILnBO,ZoL-xqz!tBZcWezdVh?e\\n&m-yVeFuB:xK.TXRYWAMJ,qvzggra.YglrMkH,-q'XY$joa$MJaOUEb'\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So now we need to create a loss Fn that can compare that output to the expected output and give use some numeric value representing how close the two were."
      ],
      "metadata": {
        "id": "owEIZhWdFqAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "64qMiaSSFVhy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the Model"
      ],
      "metadata": {
        "id": "rP-U3k-YF9CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "NRXUKVmJF8M2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll set up checkpoints as it trains that will allow us to load our model to train for later purposes if need be."
      ],
      "metadata": {
        "id": "gVeaOjVRGsdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "#name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,'ckpt_(epoch)')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "SZSG1lFOGqx0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train our Model"
      ],
      "metadata": {
        "id": "0TkZ8K-rHmj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARZWPe1yHjUJ",
        "outputId": "06e76402-16bd-4605-c33e-2d08ad675d4e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "86/86 [==============================] - 13s 120ms/step - loss: 2.9629\n",
            "Epoch 2/40\n",
            "86/86 [==============================] - 12s 121ms/step - loss: 2.1987\n",
            "Epoch 3/40\n",
            "86/86 [==============================] - 12s 124ms/step - loss: 1.9246\n",
            "Epoch 4/40\n",
            "86/86 [==============================] - 12s 124ms/step - loss: 1.7446\n",
            "Epoch 5/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.6187\n",
            "Epoch 6/40\n",
            "86/86 [==============================] - 12s 126ms/step - loss: 1.5312\n",
            "Epoch 7/40\n",
            "86/86 [==============================] - 12s 128ms/step - loss: 1.4671\n",
            "Epoch 8/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.4166\n",
            "Epoch 9/40\n",
            "86/86 [==============================] - 12s 126ms/step - loss: 1.3763\n",
            "Epoch 10/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 1.3425\n",
            "Epoch 11/40\n",
            "86/86 [==============================] - 12s 129ms/step - loss: 1.3124\n",
            "Epoch 12/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 1.2828\n",
            "Epoch 13/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.2550\n",
            "Epoch 14/40\n",
            "86/86 [==============================] - 12s 124ms/step - loss: 1.2293\n",
            "Epoch 15/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.2023\n",
            "Epoch 16/40\n",
            "86/86 [==============================] - 12s 131ms/step - loss: 1.1762\n",
            "Epoch 17/40\n",
            "86/86 [==============================] - 12s 129ms/step - loss: 1.1474\n",
            "Epoch 18/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 1.1202\n",
            "Epoch 19/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.0901\n",
            "Epoch 20/40\n",
            "86/86 [==============================] - 12s 124ms/step - loss: 1.0596\n",
            "Epoch 21/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 1.0301\n",
            "Epoch 22/40\n",
            "86/86 [==============================] - 12s 128ms/step - loss: 0.9969\n",
            "Epoch 23/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 0.9644\n",
            "Epoch 24/40\n",
            "86/86 [==============================] - 12s 128ms/step - loss: 0.9310\n",
            "Epoch 25/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 0.8984\n",
            "Epoch 26/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.8641\n",
            "Epoch 27/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.8338\n",
            "Epoch 28/40\n",
            "86/86 [==============================] - 12s 128ms/step - loss: 0.8014\n",
            "Epoch 29/40\n",
            "86/86 [==============================] - 12s 131ms/step - loss: 0.7711\n",
            "Epoch 30/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.7412\n",
            "Epoch 31/40\n",
            "86/86 [==============================] - 12s 126ms/step - loss: 0.7133\n",
            "Epoch 32/40\n",
            "86/86 [==============================] - 12s 126ms/step - loss: 0.6885\n",
            "Epoch 33/40\n",
            "86/86 [==============================] - 12s 126ms/step - loss: 0.6633\n",
            "Epoch 34/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.6409\n",
            "Epoch 35/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 0.6200\n",
            "Epoch 36/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.5995\n",
            "Epoch 37/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 0.5791\n",
            "Epoch 38/40\n",
            "86/86 [==============================] - 12s 127ms/step - loss: 0.5622\n",
            "Epoch 39/40\n",
            "86/86 [==============================] - 12s 129ms/step - loss: 0.5469\n",
            "Epoch 40/40\n",
            "86/86 [==============================] - 12s 125ms/step - loss: 0.5341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model\n",
        "We'll rebuild the model from a checkpoint using a batch_size of 1 so that we fan feed one piece of text to the model and have it make a prediction."
      ],
      "metadata": {
        "id": "QM5JP6N4H_KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "fbKoN7zmH-H1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the latest loaded checkpoint\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "k43NVZ2dIkgV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checkpoint_num = 10\n",
        "# model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\"+str(checkpoint_num)))\n",
        "# model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "NVTRqavRI-gz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating the Data"
      ],
      "metadata": {
        "id": "o1VkY1LaJUlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "tensorflow FN to generate some text using any starting string\n",
        "\"\"\"\n",
        "def generate_text(model, start_string):\n",
        "    num_gen = 800\n",
        "\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval,0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    temp = 1.0\n",
        "    model.reset_states()\n",
        "    for i in range(num_gen):\n",
        "        pred = model(input_eval)\n",
        "        pred = tf.squeeze(pred, 0)\n",
        "\n",
        "        pred = pred / temp\n",
        "        pred_id = tf.random.categorical(pred, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "        input_eval = tf.expand_dims([pred_id], 0)\n",
        "\n",
        "        text_generated.append(idx2char[pred_id])\n",
        "\n",
        "    return (start_string + \"\".join(text_generated))\n"
      ],
      "metadata": {
        "id": "0tbfAhTGJTz_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOaRuxKyLeLo",
        "outputId": "690710fb-d05f-4972-d75b-b675203bd03d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: His arms were as cold as \n",
            "His arms were as cold as chequest;\n",
            "And thou, and death will have you quake,\n",
            "Which she hath praised and fighters from the king,\n",
            "Who hath cambifian'd with a spirit to die.\n",
            "Where is your crat-moner? when did I beg them not\n",
            "In the hile own window, like power incourse.'\n",
            "Come, Warwick, through the country of your eye,\n",
            "But he is own repair, and dread none othor\n",
            "To teach her kinsman and well committed to them, if he wear fellows from right,\n",
            "But, as I can, it do change, sir, and the little tricks of\n",
            "conscience says we stand and need to Romeo's hand dustices, that they are sharl\n",
            "Can when thou art d, well known to the pun my heart. I beg the world s after to.\n",
            "Hark: the sleep deeds did return to do\n",
            "And mine own lineal with honest peace\n",
            "And not on his sword, Lord Anbelo, to you:\n",
            "And were the reis of them and clear love?\n",
            "\n",
            "JOHN \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hc6D5C5ZMQtx"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}