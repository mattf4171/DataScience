{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bih2Irvp8LFJ"
      },
      "source": [
        "# Natural Language Processing\n",
        "NLP's are a discipline in Data Science that deals with the communiaits between natural languages and computer langauges. Ac ommon example of an NLP is spellcheck or autocompelte.\n",
        "\n",
        "### Recurrent Nerual Networks\n",
        "Algorithm that is much more capable of processing sequential data such as text or characters called a recurrent neural network(RNN).\n",
        "* Sentiment Analysis\n",
        "* Character Generation \n",
        "\n",
        "### Our Data - Sequential\n",
        "This data we will look at is sequential, since we will encode our text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-4GuqYP8LFN",
        "outputId": "36dd479a-54e2-4906-e4bc-ec2666bfb280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{1: 3, 2: 3, 3: 2, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1}\n",
            "{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9, 'that': 10, 'then': 11}\n"
          ]
        }
      ],
      "source": [
        "vocab = {} # map words to integer types\n",
        "word_encoding = 1 \n",
        "def bag_of_words(text):\n",
        "    global word_encoding\n",
        "\n",
        "    words = text.lower().split(\" \")\n",
        "    bag = {}\n",
        "    for word in words:\n",
        "        if word in vocab:\n",
        "            encoding = vocab[word]\n",
        "        else:\n",
        "            vocab[word] = word_encoding\n",
        "            encoding = word_encoding\n",
        "            word_encoding += 1\n",
        "        if encoding in bag:\n",
        "            bag[encoding] += 1\n",
        "        else:\n",
        "            bag[encoding] = 1\n",
        "    return bag\n",
        "text = \"This is a test to see if this test will work is is test a this that then\"\n",
        "bag = bag_of_words(text)\n",
        "print(bag)\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm1zQZvj8LFR"
      },
      "source": [
        "The above approach of encoding our data to be numerical values(\"Bag of Words\") will have not perform well. We want our encoding to have vectors that represent very similar words to be very close to one another. In other words the angle between two vectors are small if the words are very similar. This method is called _Word Embedding_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8xCE10i8LFS",
        "outputId": "23ef8903-cbe8-4248-92c5-71a28e3b51f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x \n",
        "from keras.datasets import imdb\n",
        "# from keras.preprocessing import sequence\n",
        "from keras.utils import pad_sequences\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxHuob1H8QdB"
      },
      "outputs": [],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c84yN8K8LFT"
      },
      "source": [
        "We need our data length inputs to our neural network to be the same length. This issue can be solved as such:\n",
        "* If the review is > 250 words trim off extra words\n",
        "* If the review is < 250 words add trailing zeros to equal 250 words\n",
        "keras has a neat function to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qsznjpWQ8bEg"
      },
      "outputs": [],
      "source": [
        "train_data = pad_sequences(train_data, MAXLEN)\n",
        "test_data = pad_sequences(test_data, MAXLEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZK6EYct923g"
      },
      "source": [
        "## Building the Model\n",
        "Let us now create the model. We'll use a word embedding layer and add an LSTM layer that feeds into a Dense node to get our predicted sentiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du83Ye3X9y-g",
        "outputId": "50946fc4-7d73-44b0-8a77-17a0570630a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "tf.keras.layers.LSTM\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1,activation=\"sigmoid\") # Squash values b/w [0,1]\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSU7hajV_JK3"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY-gZmlt-bDj",
        "outputId": "a48c7953-3ac5-411d-a5bf-7d967912d23a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 72s 111ms/step - loss: 0.4049 - acc: 0.8152 - val_loss: 0.3378 - val_acc: 0.8536\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 63s 101ms/step - loss: 0.2375 - acc: 0.9103 - val_loss: 0.3098 - val_acc: 0.8820\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 64s 102ms/step - loss: 0.1851 - acc: 0.9333 - val_loss: 0.2797 - val_acc: 0.8900\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 62s 100ms/step - loss: 0.1513 - acc: 0.9459 - val_loss: 0.3017 - val_acc: 0.8940\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 62s 100ms/step - loss: 0.1298 - acc: 0.9549 - val_loss: 0.3158 - val_acc: 0.8926\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 62s 99ms/step - loss: 0.1136 - acc: 0.9606 - val_loss: 0.3485 - val_acc: 0.8858\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 63s 100ms/step - loss: 0.0951 - acc: 0.9692 - val_loss: 0.3098 - val_acc: 0.8864\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 62s 100ms/step - loss: 0.0849 - acc: 0.9724 - val_loss: 0.3328 - val_acc: 0.8890\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 62s 99ms/step - loss: 0.0724 - acc: 0.9769 - val_loss: 0.3852 - val_acc: 0.8878\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 62s 99ms/step - loss: 0.0660 - acc: 0.9801 - val_loss: 0.7093 - val_acc: 0.8390\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMEotn3I_YN7",
        "outputId": "434aa89c-16a9-4493-8e2f-3627794db984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 20s 25ms/step - loss: 0.9683 - acc: 0.7812\n",
            "Loss: 0.9682536125183105    Accuracy: 0.7811599969863892\n"
          ]
        }
      ],
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print('Loss: {}    Accuracy: {}'.format(results[0],results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVostXRR_1F8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e50035b3dfd539dbd2dca26cbba61024e04488120c7d4591534a01642a5bb0ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
